{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_weather_train = pd.read_csv(\"../ashrae-energy-prediction/train.csv\")\n",
    "df_weather_train = pd.read_csv(\"train.csv\")\n",
    "df_weather_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (assuming you've already read it)\n",
    "df_weather_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.histplot(df_weather_train[\"meter_reading\"], bins=50, kde=True)\n",
    "# plt.xlabel(\"Meter Reading\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.title(\"Distribution of Meter Readings\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_weather_train[\"meter\"].value_counts().plot(kind=\"bar\", title=\"Meter Type Distribution\")\n",
    "# plt.xlabel(\"Meter Type\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_train[\"timestamp\"] = pd.to_datetime(df_weather_train[\"timestamp\"])\n",
    "df_weather_train[\"hour\"] = df_weather_train[\"timestamp\"].dt.hour\n",
    "df_weather_train[\"day\"] = df_weather_train[\"timestamp\"].dt.day\n",
    "df_weather_train[\"month\"] = df_weather_train[\"timestamp\"].dt.month\n",
    "df_weather_train[\"year\"] = df_weather_train[\"timestamp\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_weather_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Distribution of meter readings\n",
    "# sns.histplot(df_weather_train['meter_reading'], bins=50, kde=True)\n",
    "# plt.title(\"Distribution of Meter Readings\")\n",
    "# plt.show()\n",
    "\n",
    "# # Trend analysis\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# sns.lineplot(x=df_weather_train['timestamp'], y=df_weather_train['meter_reading'])\n",
    "# plt.title(\"Meter Readings Over Time\")\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Define Generator Network\n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, output_dim)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# # Define Discriminator Network\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# # Training the GAN\n",
    "# def train_gan(real_data, epochs=1000):\n",
    "#     input_dim = real_data.shape[1]\n",
    "#     generator = Generator(input_dim, input_dim)\n",
    "#     discriminator = Discriminator(input_dim)\n",
    "#     optimizer_G = optim.Adam(generator.parameters(), lr=0.001)\n",
    "#     optimizer_D = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "#     loss_function = nn.BCELoss()\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         real_labels = torch.ones(real_data.shape[0], 1)\n",
    "#         fake_labels = torch.zeros(real_data.shape[0], 1)\n",
    "\n",
    "#         # Train Discriminator\n",
    "#         optimizer_D.zero_grad()\n",
    "#         real_predictions = discriminator(real_data)\n",
    "#         real_loss = loss_function(real_predictions, real_labels)\n",
    "\n",
    "#         noise = torch.randn(real_data.shape[0], input_dim)\n",
    "#         fake_data = generator(noise)\n",
    "#         fake_predictions = discriminator(fake_data.detach())\n",
    "#         fake_loss = loss_function(fake_predictions, fake_labels)\n",
    "\n",
    "#         d_loss = real_loss + fake_loss\n",
    "#         d_loss.backward()\n",
    "#         optimizer_D.step()\n",
    "\n",
    "#         # Train Generator\n",
    "#         optimizer_G.zero_grad()\n",
    "#         fake_predictions = discriminator(fake_data)\n",
    "#         g_loss = loss_function(fake_predictions, real_labels)\n",
    "#         g_loss.backward()\n",
    "#         optimizer_G.step()\n",
    "\n",
    "#         if epoch % 100 == 0:\n",
    "#             print(f\"Epoch {epoch}, D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n",
    "\n",
    "#     return generator\n",
    "\n",
    "# # Convert dataset to tensor and train GAN\n",
    "# real_data_tensor = torch.FloatTensor(df_weather_train[['meter_reading']].values)\n",
    "# real_data_tensor = real_data_tensor[:10000]\n",
    "# generator = train_gan(real_data_tensor, epochs=1000)\n",
    "\n",
    "# # Generate synthetic data\n",
    "# noise = torch.randn(10000, 1)\n",
    "# synthetic_meter_readings = generator(noise).detach().numpy()\n",
    "\n",
    "# print(synthetic_meter_readings[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPROVED APPROACH => P-VALUE = 0.0 (ATTEMPT 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.autograd as autograd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Data Preprocessing\n",
    "# def normalize_data(data):\n",
    "#     scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "#     return scaler, scaler.fit_transform(data)\n",
    "\n",
    "# def denormalize_data(scaler, data):\n",
    "#     return scaler.inverse_transform(data)\n",
    "\n",
    "# # Define Generator Network\n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 128),\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(128, 256),\n",
    "#             nn.BatchNorm1d(256),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(256, 512),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(512, output_dim),\n",
    "#             nn.Tanh()  # Tanh output to match normalized data range\n",
    "#         )\n",
    "#         self.apply(self._weights_init)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "#     def _weights_init(self, m):\n",
    "#         if isinstance(m, nn.Linear):\n",
    "#             nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# # Define Discriminator Network\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 512),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(128, 1)\n",
    "#         )\n",
    "#         self.apply(self._weights_init)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "#     def _weights_init(self, m):\n",
    "#         if isinstance(m, nn.Linear):\n",
    "#             nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# # Compute Gradient Penalty for WGAN-GP\n",
    "# def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "#     alpha = torch.rand(real_samples.size(0), 1).to(real_samples.device)\n",
    "#     interpolated_samples = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "#     interpolated_predictions = discriminator(interpolated_samples)\n",
    "\n",
    "#     gradients = autograd.grad(\n",
    "#         outputs=interpolated_predictions,\n",
    "#         inputs=interpolated_samples,\n",
    "#         grad_outputs=torch.ones_like(interpolated_predictions),\n",
    "#         create_graph=True,\n",
    "#         retain_graph=True,\n",
    "#         only_inputs=True\n",
    "#     )[0]\n",
    "\n",
    "#     gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "#     return gradient_penalty\n",
    "\n",
    "# # Training the WGAN-GP\n",
    "# def train_wgan_gp(real_data, epochs=1000, lambda_gp=10):\n",
    "#     input_dim = real_data.shape[1]\n",
    "#     generator = Generator(input_dim, input_dim)\n",
    "#     discriminator = Discriminator(input_dim)\n",
    "\n",
    "#     optimizer_G = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "#     optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         for _ in range(5):  # Update discriminator more frequently\n",
    "#             optimizer_D.zero_grad()\n",
    "#             real_labels = torch.ones(real_data.shape[0], 1) * 0.9  # Label Smoothing\n",
    "\n",
    "#             noise = torch.randn(real_data.shape[0], input_dim)\n",
    "#             fake_data = generator(noise).detach()\n",
    "#             fake_labels = torch.zeros(real_data.shape[0], 1)\n",
    "\n",
    "#             real_predictions = discriminator(real_data)\n",
    "#             fake_predictions = discriminator(fake_data)\n",
    "\n",
    "#             gradient_penalty = compute_gradient_penalty(discriminator, real_data, fake_data)\n",
    "#             d_loss = -(real_predictions.mean() - fake_predictions.mean()) + lambda_gp * gradient_penalty\n",
    "\n",
    "#             d_loss.backward()\n",
    "#             nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)  # Gradient Clipping\n",
    "#             optimizer_D.step()\n",
    "\n",
    "#         # Train Generator\n",
    "#         optimizer_G.zero_grad()\n",
    "#         noise = torch.randn(real_data.shape[0], input_dim)\n",
    "#         fake_data = generator(noise)\n",
    "#         fake_predictions = discriminator(fake_data)\n",
    "\n",
    "#         g_loss = -fake_predictions.mean()\n",
    "#         g_loss.backward()\n",
    "#         optimizer_G.step()\n",
    "\n",
    "#         if epoch % 100 == 0:\n",
    "#             print(f\"Epoch {epoch}, D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "#     return generator\n",
    "\n",
    "# # Convert dataset to tensor and train WGAN-GP\n",
    "# real_data_np = df_weather_train[['meter_reading']].values\n",
    "# scaler, real_data_normalized = normalize_data(real_data_np)\n",
    "# real_data_tensor = torch.FloatTensor(real_data_normalized[:10000])\n",
    "\n",
    "# generator = train_wgan_gp(real_data_tensor, epochs=1000)\n",
    "\n",
    "# # Generate synthetic data\n",
    "# noise = torch.randn(10000, 1)\n",
    "# synthetic_meter_readings_normalized = generator(noise).detach().numpy()\n",
    "# synthetic_meter_readings = denormalize_data(scaler, synthetic_meter_readings_normalized)\n",
    "\n",
    "# print(synthetic_meter_readings[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTEMPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "\n",
    "# Data Preprocessing\n",
    "def normalize_data(data):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    return scaler, scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "def denormalize_data(scaler, data):\n",
    "    return scaler.inverse_transform(data)\n",
    "\n",
    "\n",
    "# Define Swish Activation\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "# Define Generator Network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            Swish(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            Swish(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            Swish(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            Swish(),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.apply(self._weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight, gain=0.1)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "# Define Discriminator Network with Dropout and Noise\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),  # Dropout for regularization\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.apply(self._weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        noise = 0.1 * torch.randn_like(x)  # Adding noise to input for stability\n",
    "        x_noisy = x + noise\n",
    "        return self.model(x_noisy)\n",
    "\n",
    "    def _weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight, gain=0.1)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "# Compute Gradient Penalty\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "    alpha = torch.rand(real_samples.size(0), 1).to(real_samples.device)\n",
    "    interpolated_samples = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    interpolated_predictions = discriminator(interpolated_samples)\n",
    "    gradients = autograd.grad(\n",
    "        outputs=interpolated_predictions,\n",
    "        inputs=interpolated_samples,\n",
    "        grad_outputs=torch.ones_like(interpolated_predictions),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "# Training WGAN-GP\n",
    "def train_wgan_gp(real_data, epochs=7000, lambda_gp=10, early_stop_ks=0.1, patience=10):\n",
    "    input_dim = real_data.shape[1]\n",
    "    generator = Generator(input_dim, input_dim)\n",
    "    discriminator = Discriminator(input_dim)\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "\n",
    "    # Removed verbose to avoid warning\n",
    "    scheduler_G = optim.lr_scheduler.ReduceLROnPlateau(optimizer_G, mode='min', patience=3, factor=0.5)\n",
    "    scheduler_D = optim.lr_scheduler.ReduceLROnPlateau(optimizer_D, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "    d_losses, g_losses, ks_scores = [], [], []\n",
    "    best_ks_score = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(5):  # Train discriminator more frequently\n",
    "            optimizer_D.zero_grad()\n",
    "            noise = torch.randn(real_data.shape[0], input_dim) * 0.5\n",
    "            fake_data = generator(noise).detach()\n",
    "            real_predictions = discriminator(real_data)\n",
    "            fake_predictions = discriminator(fake_data)\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_data, fake_data)\n",
    "            d_loss = -(real_predictions.mean() - fake_predictions.mean()) + lambda_gp * gradient_penalty\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        noise = torch.randn(real_data.shape[0], input_dim) * 0.5\n",
    "        fake_data = generator(noise)\n",
    "        fake_predictions = discriminator(fake_data)\n",
    "        g_loss = -fake_predictions.mean()\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        d_losses.append(d_loss.item())\n",
    "        g_losses.append(g_loss.item())\n",
    "\n",
    "        # Perform KS test to monitor similarity\n",
    "        if epoch % 500 == 0:\n",
    "            synthetic_data = generator(noise).detach().cpu().numpy()\n",
    "            ks_stat, ks_p_value = ks_2samp(real_data.numpy().flatten(), synthetic_data.flatten())\n",
    "            ks_scores.append(ks_stat)\n",
    "            print(f\"Epoch {epoch}, D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}, KS P-Value: {ks_p_value:.4f}\")\n",
    "\n",
    "            # Update learning rate using scheduler\n",
    "            scheduler_G.step(ks_stat)\n",
    "            scheduler_D.step(ks_stat)\n",
    "\n",
    "            # Print the current learning rates\n",
    "            print(f\"Generator LR: {scheduler_G.optimizer.param_groups[0]['lr']}\")\n",
    "            print(f\"Discriminator LR: {scheduler_D.optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "            # Early stopping based on KS similarity improvement\n",
    "            if ks_stat < best_ks_score:\n",
    "                best_ks_score = ks_stat\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if ks_p_value > early_stop_ks or patience_counter >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch} due to KS similarity or patience limit!\")\n",
    "                break\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(d_losses, label='D Loss')\n",
    "    plt.plot(g_losses, label='G Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training Loss')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(ks_scores, label='KS Score')\n",
    "    plt.legend()\n",
    "    plt.title('KS Test Score')\n",
    "    plt.show()\n",
    "\n",
    "    return generator\n",
    "\n",
    "\n",
    "# Load and Normalize Dataset\n",
    "real_data_np = df_weather_train[['meter_reading']].values\n",
    "scaler, real_data_normalized = normalize_data(real_data_np)\n",
    "real_data_tensor = torch.FloatTensor(real_data_normalized[:100])  # Use first 100 values for training\n",
    "\n",
    "# Train WGAN-GP\n",
    "generator = train_wgan_gp(real_data_tensor, epochs=7000, lambda_gp=10, early_stop_ks=0.1)\n",
    "\n",
    "# Generate Synthetic Data\n",
    "noise = torch.randn(10000, 1) * 0.5\n",
    "synthetic_meter_readings_normalized = generator(noise).detach().numpy()\n",
    "synthetic_meter_readings = denormalize_data(scaler, synthetic_meter_readings_normalized)\n",
    "\n",
    "print(synthetic_meter_readings[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution based evaluation using histogram and KDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ORIGINAL HISTOGRAM AND KDE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot histogram and KDE for real and synthetic data\n",
    "# plt.figure(figsize=(10,5))\n",
    "# sns.kdeplot(real_data_tensor.numpy().flatten(), label=\"Real Data\", fill=True, alpha=0.5)\n",
    "# sns.kdeplot(synthetic_meter_readings.flatten(), label=\"Synthetic Data\", fill=True, alpha=0.5)\n",
    "# plt.xlabel(\"Meter Reading\")\n",
    "# plt.ylabel(\"Density\")\n",
    "# plt.legend()\n",
    "# plt.title(\"Comparison of Real and Synthetic Meter Readings\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRIAL HISTOGRAM AND KDE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram and KDE for real and synthetic data\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.kdeplot(real_data_tensor.numpy().flatten(), label=\"Real Data\", fill=True, alpha=0.5, warn_singular=False)\n",
    "sns.kdeplot(synthetic_meter_readings.flatten(), label=\"Synthetic Data\", fill=True, alpha=0.5, warn_singular=False)\n",
    "plt.xlabel(\"Meter Reading\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparison of Real and Synthetic Meter Readings\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KS Test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS Test Evaluation\n",
    "ks_stat, ks_p_value = ks_2samp(real_data_tensor.numpy().flatten(), synthetic_meter_readings.flatten())\n",
    "print(f\"Final KS Statistic: {ks_stat}\")\n",
    "print(f\"Final P-Value: {ks_p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "if ks_p_value > 0.05:\n",
    "    print(\"The synthetic data distribution is similar to the real data distribution (Fail to reject H0).\")\n",
    "else:\n",
    "    print(\"The synthetic data distribution differs significantly from the real data distribution (Reject H0).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Train Autoencoder\n",
    "def train_autoencoder(autoencoder, data, epochs=100, batch_size=256, learning_rate=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    dataset = torch.utils.data.TensorDataset(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            batch_data = batch[0]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(batch_data)\n",
    "            loss = criterion(reconstructed, batch_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {total_loss / len(dataloader):.6f}\")\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "# Compute Reconstruction Error\n",
    "def compute_reconstruction_error(autoencoder, data):\n",
    "    with torch.no_grad():\n",
    "        reconstructed = autoencoder(data)\n",
    "        errors = torch.mean((data - reconstructed) ** 2, dim=1)\n",
    "    return errors.numpy()\n",
    "\n",
    "# Normalize Real Data and Train Autoencoder\n",
    "scaler, real_data_normalized = normalize_data(df_weather_train[['meter_reading']].values)\n",
    "real_data_tensor = torch.FloatTensor(real_data_normalized[:10000])\n",
    "\n",
    "autoencoder = Autoencoder(input_dim=1)\n",
    "autoencoder = train_autoencoder(autoencoder, real_data_tensor, epochs=100)\n",
    "\n",
    "# Compute Reconstruction Error for Real and Synthetic Data\n",
    "real_errors = compute_reconstruction_error(autoencoder, real_data_tensor)\n",
    "synthetic_data_tensor = torch.FloatTensor(synthetic_meter_readings_normalized)\n",
    "synthetic_errors = compute_reconstruction_error(autoencoder, synthetic_data_tensor)\n",
    "\n",
    "# Plot Reconstruction Errors\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.kdeplot(real_errors, label=\"Real Data\", fill=True, alpha=0.5)\n",
    "sns.kdeplot(synthetic_errors, label=\"Synthetic Data\", fill=True, alpha=0.5)\n",
    "plt.xlabel(\"Reconstruction Error\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparison of Reconstruction Errors for Real and Synthetic Data\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iotenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
