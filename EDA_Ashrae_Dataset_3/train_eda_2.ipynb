{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "wp69YXX5DeQX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb20axhfFoia",
        "outputId": "8d69ffbf-435d-440c-b7cc-8027c34f6ee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FtvY5tXOl4r",
        "outputId": "1f08e728-a0fb-41ab-e308-eabdac7251b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, D Loss: 8.6676, G Loss: 0.1803\n",
            "Epoch 500, D Loss: 4.6302, G Loss: -5.0625\n",
            "Epoch 1000, D Loss: 9.2659, G Loss: -6.1544\n",
            "Epoch 1500, D Loss: 13.0292, G Loss: -4.6330\n",
            "Epoch 2000, D Loss: -18.2859, G Loss: 31.8388\n",
            "Epoch 2500, D Loss: -9.6847, G Loss: 14.1566\n",
            "Epoch 3000, D Loss: -4.4446, G Loss: 3.8311\n",
            "Epoch 3500, D Loss: -1.2415, G Loss: -4.1905\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import wasserstein_distance\n",
        "from scipy.spatial.distance import euclidean\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/IOT PE 6th semester/Till 5th meet/train.csv\")\n",
        "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
        "df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
        "df[\"day\"] = df[\"timestamp\"].dt.day\n",
        "df[\"month\"] = df[\"timestamp\"].dt.month\n",
        "df[\"year\"] = df[\"timestamp\"].dt.year\n",
        "\n",
        "# Select features and target\n",
        "features = [\"building_id\", \"meter\", \"hour\", \"day\", \"month\", \"year\"]\n",
        "target = \"meter_reading\"\n",
        "\n",
        "# Normalize data\n",
        "scaler_x = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "x_data = scaler_x.fit_transform(df[features].values)\n",
        "y_data = scaler_y.fit_transform(df[[target]].values)\n",
        "\n",
        "# Convert to tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "x_tensor = torch.FloatTensor(x_data).to(device)\n",
        "y_tensor = torch.FloatTensor(y_data).to(device)\n",
        "\n",
        "# Define Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, condition_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim + condition_dim, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, condition):\n",
        "        x = torch.cat([noise, condition], dim=1)\n",
        "        return self.model(x)\n",
        "\n",
        "# Define Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim, condition_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim + condition_dim, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, data, condition):\n",
        "        x = torch.cat([data, condition], dim=1)\n",
        "        return self.model(x)\n",
        "\n",
        "# Gradient penalty\n",
        "def gradient_penalty(discriminator, real_data, fake_data, condition):\n",
        "    alpha = torch.rand(real_data.size(0), 1, device=device)\n",
        "    interpolates = (alpha * real_data + (1 - alpha) * fake_data).requires_grad_(True)\n",
        "    d_interpolates = discriminator(interpolates, condition)\n",
        "    gradients = torch.autograd.grad(outputs=d_interpolates, inputs=interpolates,\n",
        "                                    grad_outputs=torch.ones_like(d_interpolates),\n",
        "                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "\n",
        "# Training function\n",
        "def train_gan(epochs=5000, batch_size=1024, lambda_gp=15):\n",
        "    input_dim, condition_dim, output_dim = 1, x_tensor.shape[1], 1\n",
        "    generator = Generator(input_dim, condition_dim, output_dim).to(device)\n",
        "    discriminator = Discriminator(input_dim, condition_dim).to(device)\n",
        "\n",
        "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
        "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for _ in range(7):  # More D updates per G update\n",
        "            idx = np.random.randint(0, x_tensor.shape[0], batch_size)\n",
        "            real_x, real_y = x_tensor[idx], y_tensor[idx]\n",
        "            noise = torch.randn(batch_size, input_dim, device=device)\n",
        "            fake_y = generator(noise, real_x).detach()\n",
        "\n",
        "            optimizer_D.zero_grad()\n",
        "            real_pred = discriminator(real_y, real_x) + 0.01 * torch.randn_like(real_y)  # Add noise to real data\n",
        "            fake_pred = discriminator(fake_y, real_x)\n",
        "            gp = gradient_penalty(discriminator, real_y, fake_y, real_x)\n",
        "            d_loss = -torch.mean(real_pred) + torch.mean(fake_pred) + lambda_gp * gp\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        noise = torch.randn(batch_size, input_dim, device=device)\n",
        "        fake_y = generator(noise, real_x)\n",
        "        fake_pred = discriminator(fake_y, real_x)\n",
        "        g_loss = -torch.mean(fake_pred)\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            print(f\"Epoch {epoch}, D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "    return generator\n",
        "\n",
        "# Train GAN\n",
        "generator = train_gan()\n",
        "\n",
        "# Generate synthetic data\n",
        "num_samples = len(x_tensor)\n",
        "synthetic_y = generator(torch.randn(num_samples, 1, device=device), x_tensor).detach().cpu().numpy()\n",
        "synthetic_y = scaler_y.inverse_transform(synthetic_y)\n",
        "\n",
        "# Evaluation\n",
        "real_values = y_tensor.cpu().numpy().flatten()\n",
        "synthetic_values = synthetic_y.flatten()\n",
        "\n",
        "print(f\"Wasserstein Distance: {wasserstein_distance(real_values, synthetic_values):.4f}\")\n",
        "print(f\"Frechet Distance: {euclidean([real_values.mean()], [synthetic_values.mean()]):.4f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mean_absolute_error(real_values, synthetic_values):.4f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mean_squared_error(real_values, synthetic_values):.4f}\")\n",
        "\n",
        "# KDE plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.kdeplot(real_values, label=\"Real Data\", fill=True, alpha=0.5, warn_singular=False)\n",
        "sns.kdeplot(synthetic_values, label=\"Synthetic Data\", fill=True, alpha=0.5, warn_singular=False)\n",
        "plt.xlabel(\"Meter Reading\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.title(\"Comparison of Real and Synthetic Meter Readings\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2yJNNisUU3_",
        "outputId": "b506cd2c-523c-46ad-c20e-9240e6eccf00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KS Statistic: 0.9912, P-value: 0.0000\n",
            "The synthetic data significantly differs from the real data (reject H0).\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import ks_2samp\n",
        "\n",
        "# KS Test Evaluation\n",
        "def ks_test_evaluation(real_data, synthetic_data):\n",
        "    real_values = real_data.cpu().numpy().flatten()\n",
        "    synthetic_values = synthetic_data.flatten()\n",
        "    ks_stat, p_value = ks_2samp(real_values, synthetic_values)\n",
        "    print(f\"KS Statistic: {ks_stat:.4f}, P-value: {p_value:.4f}\")\n",
        "    if p_value > 0.05:\n",
        "        print(\"The synthetic data follows a similar distribution to the real data (fail to reject H0).\")\n",
        "    else:\n",
        "        print(\"The synthetic data significantly differs from the real data (reject H0).\")\n",
        "\n",
        "# Perform KS test\n",
        "ks_test_evaluation(y_tensor, synthetic_y)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "iotenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
