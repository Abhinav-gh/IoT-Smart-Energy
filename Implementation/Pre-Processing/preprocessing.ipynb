{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "import sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_name):\n",
    "    \"\"\"Loads and formats data from manually specified file paths\"\"\"\n",
    "    base_path = \"../Prepared_Data/\"\n",
    "    file_paths = {\n",
    "        \"train\": \"./train.feather\",\n",
    "        \"test\": \"./test.feather\",\n",
    "        \"meta\": \"./building_metadata.feather\",\n",
    "        \"weather_train\": \"./weather_train.feather\",\n",
    "        \"weather_test\": \"./weather_test.feather\"\n",
    "    }\n",
    "    return pd.read_feather(base_path+file_paths[data_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceDataSet(train, test, building_meta, weather_train, weather_test):\n",
    "    \"\"\"Keep only the first 290 buildings by building id. It also prints how many\n",
    "    rows were removed every time it is called.\"\"\"\n",
    "    numRows = len(train)\n",
    "    train = train[train.building_id < 290]\n",
    "    print(f\"INFO: Removed {numRows - len(train)} rows from train dataset.\")\n",
    "    numRows = len(test)\n",
    "    test = test[test.building_id < 290]\n",
    "    print(f\"INFO: Removed {numRows - len(test)} rows from test dataset.\")\n",
    "\n",
    "    numRows = len(building_meta)\n",
    "    building_meta = building_meta[building_meta.building_id < 290]\n",
    "    print(f\"INFO: Removed {numRows - len(building_meta)} rows from building metadata.\")\n",
    "\n",
    "    # The first 290 buildings actually exists in site_id 1 and site_id 2.\n",
    "    # So remove all site_id > 2 from weather_train and weather_test\n",
    "    numRows = len(weather_train)\n",
    "    weather_train = weather_train[weather_train.site_id < 3]\n",
    "    print(f\"INFO: Removed {numRows - len(weather_train)} rows from train weather dataset.\")\n",
    "    numRows = len(weather_test)\n",
    "    weather_test = weather_test[weather_test.site_id < 3]\n",
    "    print(f\"INFO: Removed {numRows - len(weather_test)} rows from test weather dataset.\")\n",
    "\n",
    "    return train, test, building_meta, weather_train, weather_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define groupings and corresponding priors\n",
    "groups_and_priors = {\n",
    "    (\"hour\",):        None,\n",
    "    (\"weekday\",):     None,\n",
    "    (\"month\",):       None,\n",
    "    (\"building_id\",): None,\n",
    "    (\"primary_use\",): None,\n",
    "    (\"site_id\",):     None,    \n",
    "    (\"meter\",):       None,\n",
    "    (\"meter\", \"hour\"):        [\"gte_meter\", \"gte_hour\"],\n",
    "    (\"meter\", \"weekday\"):     [\"gte_meter\", \"gte_weekday\"],\n",
    "    (\"meter\", \"month\"):       [\"gte_meter\", \"gte_month\"],\n",
    "    (\"meter\", \"building_id\"): [\"gte_meter\", \"gte_building_id\"],\n",
    "    (\"meter\", \"primary_use\"): [\"gte_meter\", \"gte_primary_use\"],\n",
    "    (\"meter\", \"site_id\"):     [\"gte_meter\", \"gte_site_id\"],\n",
    "    (\"meter\", \"building_id\", \"hour\"):    [\"gte_meter_building_id\", \"gte_meter_hour\"],\n",
    "    (\"meter\", \"building_id\", \"weekday\"): [\"gte_meter_building_id\", \"gte_meter_weekday\"],\n",
    "    (\"meter\", \"building_id\", \"month\"):   [\"gte_meter_building_id\", \"gte_meter_month\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_timestamp(df): \n",
    "    df.timestamp = pd.to_datetime(df.timestamp)\n",
    "    df.timestamp = (df.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() // 3600\n",
    "\n",
    "\n",
    "def process_weather(df, dataset, fix_timestamps=True, interpolate_na=True, add_na_indicators=True):\n",
    "    if fix_timestamps:\n",
    "        site_GMT_offsets = [-5, 0, -7, -5, -8, 0, -5, -5, -5, -6, -7, -5, 0, -6, -5, -5]\n",
    "        GMT_offset_map = {site: offset for site, offset in enumerate(site_GMT_offsets)}\n",
    "        df.timestamp = df.timestamp + df.site_id.map(GMT_offset_map)\n",
    "\n",
    "    if interpolate_na:\n",
    "        site_dfs = []\n",
    "        unique_sites = df.site_id.unique()\n",
    "        \n",
    "        for site_id in tqdm(unique_sites, desc=\"Processing Weather Data\", unit=\"site\"):\n",
    "            site_df = df[df.site_id == site_id].set_index(\"timestamp\").reindex(\n",
    "                range(8784 if dataset == \"train\" else 8784, 26304)\n",
    "            )\n",
    "            site_df.site_id = site_id\n",
    "            for col in tqdm([c for c in site_df.columns if c != \"site_id\"], desc=f\"Interpolating Site {site_id}\", leave=False):\n",
    "                if add_na_indicators:\n",
    "                    site_df[f\"had_{col}\"] = ~site_df[col].isna()\n",
    "                site_df[col] = site_df[col].interpolate(\n",
    "                    limit_direction=\"both\", method=\"spline\", order=3\n",
    "                ).fillna(df[col].median())\n",
    "\n",
    "            site_dfs.append(site_df)\n",
    "\n",
    "        df = pd.concat(site_dfs).reset_index()\n",
    "\n",
    "    if add_na_indicators:\n",
    "        for col in tqdm(df.columns, desc=\"Adding NA Indicators\"):\n",
    "            if df[col].isna().any():\n",
    "                df[f\"had_{col}\"] = ~df[col].isna()\n",
    "\n",
    "    return df.fillna(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_feature(df, window=3, group_cols=\"site_id\", lag_cols=[\"air_temperature\"]):\n",
    "    rolled = df.groupby(group_cols)[lag_cols].rolling(window=window, min_periods=0, center=True)\n",
    "    lag_mean = rolled.mean().reset_index().astype(np.float16)\n",
    "    lag_max = rolled.quantile(0.95).reset_index().astype(np.float16)\n",
    "    lag_min = rolled.quantile(0.05).reset_index().astype(np.float16)\n",
    "    lag_std = rolled.std().reset_index().astype(np.float16)\n",
    "    for col in lag_cols:\n",
    "        df[f\"{col}_mean_lag{window}\"] = lag_mean[col]\n",
    "        df[f\"{col}_max_lag{window}\"] = lag_max[col]\n",
    "        df[f\"{col}_min_lag{window}\"] = lag_min[col]\n",
    "        df[f\"{col}_std_lag{window}\"] = lag_std[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df[\"hour\"] = df.ts.dt.hour\n",
    "    df[\"weekday\"] = df.ts.dt.weekday\n",
    "    df[\"month\"] = df.ts.dt.month\n",
    "    df[\"year\"] = df.ts.dt.year    \n",
    "    df[\"weekday_hour\"] = df.weekday.astype(str) + \"-\" + df.hour.astype(str)\n",
    "    df[\"hour_x\"] = np.cos(2*np.pi*df.timestamp/24)\n",
    "    df[\"hour_y\"] = np.sin(2*np.pi*df.timestamp/24)\n",
    "    df[\"month_x\"] = np.cos(2*np.pi*df.timestamp/(30.4*24))\n",
    "    df[\"month_y\"] = np.sin(2*np.pi*df.timestamp/(30.4*24))\n",
    "    df[\"weekday_x\"] = np.cos(2*np.pi*df.timestamp/(7*24))\n",
    "    df[\"weekday_y\"] = np.sin(2*np.pi*df.timestamp/(7*24))\n",
    "    df[\"year_built\"] = df[\"year_built\"]-1900\n",
    "    bm_ = df.building_id.astype(str) + \"-\" + df.meter.astype(str) + \"-\"\n",
    "    df[\"building_weekday_hour\"] = bm_ + df.weekday_hour\n",
    "    df[\"building_weekday\"] = bm_ + df.weekday.astype(str)\n",
    "    df[\"building_month\"] = bm_ + df.month.astype(str)\n",
    "    df[\"building_hour\"] = bm_ + df.hour.astype(str)    \n",
    "    df[\"building_meter\"] = bm_\n",
    "    dates_range = pd.date_range(start=\"2015-12-31\", end=\"2019-01-01\")\n",
    "    us_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())    \n",
    "    df[\"is_holiday\"] = (df.ts.dt.date.astype(\"datetime64\").isin(us_holidays)).astype(np.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printInfo(train, test, weather_train, weather_test, building_meta):\n",
    "    # Print the dataset information\n",
    "    print(\"Train dataset:\")\n",
    "    print(train.info())\n",
    "    print(\"Test dataset:\")\n",
    "    print(test.info())\n",
    "    print(\"Building Metadata:\")\n",
    "    print(building_meta.info())\n",
    "    print(\"Train Weather:\")\n",
    "    print(weather_train.info())\n",
    "    print(\"Test Weather:\")\n",
    "    print(weather_test.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "train = load_data(\"train\")\n",
    "test = load_data(\"test\")\n",
    "building_meta = load_data(\"meta\")\n",
    "train_weather = load_data(\"weather_train\")\n",
    "test_weather = load_data(\"weather_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the dataset for faster pre-processing. But only if faster flag is true\n",
    "reduce= False\n",
    "n = len(sys.argv)\n",
    "if(n>1):\n",
    "    if(sys.argv[1]==\"--faster\"):\n",
    "        reduce = True\n",
    "        print(\"Reducing Dataset option enabled. Reducing dataset for faster pre-processing.\")\n",
    "        train, test, building_meta, train_weather, test_weather = reduceDataSet(train, test, building_meta, train_weather, test_weather)\n",
    "        print(\"Dataset reduced.\")\n",
    "    else:\n",
    "        print(\"Invalid argument. Please use --faster to reduce dataset for faster pre-processing.\")\n",
    "        sys.exit()\n",
    "else:\n",
    "    print(\"Reducing Dataset option not enabled. Proceeding with full dataset.\")\n",
    "\n",
    "if reduce:\n",
    "    printInfo(train, test, train_weather, test_weather, building_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "print(f\"INFO: Pre-processing started.\")\n",
    "print(f\"UPDATE: Processing timestamp.\")\n",
    "process_timestamp(train)\n",
    "process_timestamp(test)\n",
    "process_timestamp(train_weather)\n",
    "process_timestamp(test_weather)\n",
    "\n",
    "print(f\"UPDATE: Processing weather.\")\n",
    "process_weather(train_weather, \"train\")\n",
    "process_weather(test_weather, \"test\")\n",
    "\n",
    "print(f\"UPDATE: Adding lag features.\")\n",
    "for window_size in [7, 73]:\n",
    "    add_lag_feature(train_weather, window=window_size)\n",
    "    add_lag_feature(test_weather, window=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "print(f\"UPDATE: Merging datasets.\")\n",
    "train = pd.merge(train, building_meta, \"left\", \"building_id\")\n",
    "train = pd.merge(train, train_weather, \"left\", [\"site_id\", \"timestamp\"])\n",
    "test = pd.merge(test, building_meta, \"left\", \"building_id\")\n",
    "test = pd.merge(test, test_weather, \"left\", [\"site_id\", \"timestamp\"])\n",
    "\n",
    "# Add features\n",
    "print(f\"UPDATE: Adding features.\")\n",
    "add_features(train)\n",
    "add_features(test)\n",
    "gc.collect()\n",
    "train.info()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "train.to_feather(\"../Processed_Data/train_processed.feather\")\n",
    "test.to_feather(\"../Processed_Data/test_processed.feather\")\n",
    "\n",
    "print(\"Preprocessing complete! Processed data saved.\")\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
