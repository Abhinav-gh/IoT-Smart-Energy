{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('train.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Filter data for building_id = 1 and sum meter_reading values for all meters at each timestamp\n",
    "data = data[data['building_id'] == 1].groupby('timestamp', as_index=False).agg({'meter_reading': 'sum'}).reset_index(drop=True)\n",
    "\n",
    "# Sort data by timestamp\n",
    "data = data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "# Extract cyclical time features\n",
    "def create_time_features(df):\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['day'] = df['timestamp'].dt.day\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "    # Create cyclical features\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
    "    df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "data = create_time_features(data)\n",
    "\n",
    "# Apply differencing to make the series stationary\n",
    "def apply_differencing(df, col):\n",
    "    df['diff_meter_reading'] = df[col].diff().fillna(0)\n",
    "    return df\n",
    "\n",
    "data = apply_differencing(data, 'meter_reading')\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "feature_cols = ['diff_meter_reading', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
    "                'month_sin', 'month_cos', 'weekday_sin', 'weekday_cos']\n",
    "\n",
    "data[feature_cols] = scaler.fit_transform(data[feature_cols])\n",
    "\n",
    "# Prepare training sequences\n",
    "sequence_length = 24  # Using 24-hour sequences for training\n",
    "\n",
    "def create_sequences(df, seq_len):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - seq_len):\n",
    "        seq = df.iloc[i:i + seq_len][feature_cols].values\n",
    "        X.append(seq)\n",
    "        y.append(df.iloc[i + seq_len]['diff_meter_reading'])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Generate training sequences\n",
    "X_train, y_train = create_sequences(data, sequence_length)\n",
    "\n",
    "print(f\"Training data shape: X_train = {X_train.shape}, y_train = {y_train.shape}\")\n",
    "\n",
    "# Define TimeGAN components\n",
    "def build_generator(latent_dim, seq_len, feature_dim):\n",
    "    inputs = Input(shape=(seq_len, latent_dim))\n",
    "    x = LSTM(128, return_sequences=True)(inputs)\n",
    "    x = LSTM(128, return_sequences=True)(x)\n",
    "    outputs = Dense(feature_dim)(x)\n",
    "    return Model(inputs, outputs, name='Generator')\n",
    "\n",
    "def build_discriminator(seq_len, feature_dim):\n",
    "    inputs = Input(shape=(seq_len, feature_dim))\n",
    "    x = LSTM(128, return_sequences=False)(inputs)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    return Model(inputs, outputs, name='Discriminator')\n",
    "\n",
    "def build_embedder(seq_len, feature_dim):\n",
    "    inputs = Input(shape=(seq_len, feature_dim))\n",
    "    x = LSTM(128, return_sequences=True)(inputs)\n",
    "    x = LSTM(128, return_sequences=False)(x)\n",
    "    outputs = Dense(128, activation='relu')(x)\n",
    "    return Model(inputs, outputs, name='Embedder')\n",
    "\n",
    "# Set dimensions\n",
    "latent_dim = 10\n",
    "feature_dim = X_train.shape[2]\n",
    "\n",
    "# Build models\n",
    "generator = build_generator(latent_dim, sequence_length, feature_dim)\n",
    "discriminator = build_discriminator(sequence_length, feature_dim)\n",
    "embedder = build_embedder(sequence_length, feature_dim)\n",
    "\n",
    "# Compile models\n",
    "generator.compile(optimizer='adam', loss='mse')\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "embedder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Track losses for visualization\n",
    "d_losses_real, d_losses_fake, g_losses = [], [], []\n",
    "\n",
    "# Train TimeGAN with hyperparameter tuning\n",
    "epochs = 300  # Increased epochs for better convergence\n",
    "batch_size = 64  # Increased batch size for smoother training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Sample noise and generate synthetic data\n",
    "    noise = np.random.normal(0, 1, (batch_size, sequence_length, latent_dim))\n",
    "    generated_data = generator.predict(noise)\n",
    "\n",
    "    # Get real data\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    real_data = X_train[idx]\n",
    "\n",
    "    # Train discriminator\n",
    "    d_loss_real = discriminator.train_on_batch(real_data, np.ones((batch_size, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch(generated_data, np.zeros((batch_size, 1)))\n",
    "\n",
    "    # Train generator\n",
    "    noise = np.random.normal(0, 1, (batch_size, sequence_length, latent_dim))\n",
    "    g_loss = generator.train_on_batch(noise, real_data)\n",
    "\n",
    "    # Save losses for visualization\n",
    "    d_losses_real.append(d_loss_real)\n",
    "    d_losses_fake.append(d_loss_fake)\n",
    "    g_losses.append(g_loss)\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, D Loss Real: {d_loss_real}, D Loss Fake: {d_loss_fake}, G Loss: {g_loss}\")\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(d_losses_real, label='D Loss Real')\n",
    "plt.plot(d_losses_fake, label='D Loss Fake')\n",
    "plt.plot(g_losses, label='G Loss')\n",
    "plt.title('TimeGAN Training Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_synthetic_data(generator, seq_len, latent_dim, scaler, feature_dim, n_samples):\n",
    "    noise = np.random.normal(0, 1, (n_samples, seq_len, latent_dim))\n",
    "    synthetic_data = generator.predict(noise)\n",
    "\n",
    "    # Reshape and inverse transform\n",
    "    synthetic_data_reshaped = synthetic_data.reshape(-1, feature_dim)\n",
    "    synthetic_data_reshaped = scaler.inverse_transform(synthetic_data_reshaped)\n",
    "    synthetic_data_reshaped = synthetic_data_reshaped[:, 0]  # Extract diff_meter_reading\n",
    "    return synthetic_data_reshaped[:len(data) - seq_len]  # Match original length\n",
    "\n",
    "# Generate multiple synthetic samples for verification\n",
    "n_samples = len(data) - sequence_length\n",
    "synthetic_samples = []\n",
    "n_generations = 5  # Generate 5 synthetic datasets for validation\n",
    "\n",
    "for _ in range(n_generations):\n",
    "    synthetic_samples.append(generate_synthetic_data(generator, sequence_length, latent_dim, scaler, feature_dim, n_samples))\n",
    "\n",
    "# Apply inverse differencing with proper length\n",
    "def apply_inverse_differencing(original_series, diff_series):\n",
    "    restored_series = [original_series[0]]  # Start with initial value\n",
    "    for diff in diff_series:\n",
    "        restored_series.append(restored_series[-1] + diff)\n",
    "    return np.array(restored_series[:len(original_series)])  # Ensure proper length\n",
    "\n",
    "# Restore synthetic meter_reading values for each generation\n",
    "original_series = data['meter_reading'].values[:len(synthetic_samples[0])]\n",
    "synthetic_meter_readings = [apply_inverse_differencing(original_series, sample) for sample in synthetic_samples]\n",
    "\n",
    "# Evaluate performance for each synthetic dataset\n",
    "def evaluate_performance(real_series, synthetic_series_list):\n",
    "    real_series_trimmed = real_series[:len(synthetic_series_list[0])]\n",
    "    \n",
    "    for i, synthetic_series in enumerate(synthetic_series_list):\n",
    "        synthetic_series_trimmed = synthetic_series[:len(real_series_trimmed)]\n",
    "        \n",
    "        mae = mean_absolute_error(real_series_trimmed, synthetic_series_trimmed)\n",
    "        mse = mean_squared_error(real_series_trimmed, synthetic_series_trimmed)\n",
    "        \n",
    "        print(f\"Generation {i + 1}: MAE: {mae:.4f}, MSE: {mse:.4f}\")\n",
    "        \n",
    "        plt.plot(real_series_trimmed, label='Real Data', alpha=0.8)\n",
    "        plt.plot(synthetic_series_trimmed, label=f'Synthetic Data {i + 1}', alpha=0.8)\n",
    "        plt.title(f'Real vs Synthetic Meter Reading - Generation {i + 1}')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Meter Reading')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Evaluate and visualize results\n",
    "evaluate_performance(original_series, synthetic_meter_readings)\n",
    "\n",
    "def visualize_pca_tsne(real_data, synthetic_data_list, feature_cols):\n",
    "    # Prepare combined dataset for PCA and t-SNE\n",
    "    real_data_flat = real_data.reshape(-1, len(feature_cols))\n",
    "\n",
    "    # Expand synthetic data to match feature dimensions\n",
    "    synthetic_data_expanded = []\n",
    "    for syn in synthetic_data_list:\n",
    "        syn_expanded = np.tile(syn.reshape(-1, 1), (1, len(feature_cols)))  # Replicate across feature columns\n",
    "        synthetic_data_expanded.append(syn_expanded)\n",
    "\n",
    "    synthetic_data_flat = np.vstack(synthetic_data_expanded)  # Stack all synthetic samples\n",
    "\n",
    "    combined_data = np.vstack([real_data_flat, synthetic_data_flat])\n",
    "    labels = np.array([0] * len(real_data_flat) + [1] * len(synthetic_data_flat))\n",
    "\n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(combined_data)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(pca_result[labels == 0, 0], pca_result[labels == 0, 1], label='Real Data', alpha=0.6)\n",
    "    plt.scatter(pca_result[labels == 1, 0], pca_result[labels == 1, 1], label='Synthetic Data', alpha=0.6)\n",
    "    plt.title('PCA: Real vs Synthetic Data')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_result = tsne.fit_transform(combined_data)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(tsne_result[labels == 0, 0], tsne_result[labels == 0, 1], label='Real Data', alpha=0.6)\n",
    "    plt.scatter(tsne_result[labels == 1, 0], tsne_result[labels == 1, 1], label='Synthetic Data', alpha=0.6)\n",
    "    plt.title('t-SNE: Real vs Synthetic Data')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Call function after generating synthetic data\n",
    "visualize_pca_tsne(X_train, synthetic_meter_readings, feature_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iotenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
