{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, GRU, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load CSV file\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# Select relevant columns\n",
    "data = data[['building_id', 'meter', 'timestamp', 'meter_reading']]\n",
    "\n",
    "# Normalize meter_reading\n",
    "scaler = MinMaxScaler()\n",
    "data['meter_reading'] = scaler.fit_transform(data[['meter_reading']])\n",
    "\n",
    "# Group by building_id and meter to create time series\n",
    "grouped = data.groupby(['building_id', 'meter'])\n",
    "\n",
    "# Create sequences of time series data\n",
    "def create_sequences(group, sequence_length=24):\n",
    "    sequences = []\n",
    "    for i in range(len(group) - sequence_length):\n",
    "        seq = group['meter_reading'].iloc[i:i+sequence_length].values\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "# Generate sequences\n",
    "sequences_list = []\n",
    "for _, group in grouped:\n",
    "    seq = create_sequences(group)\n",
    "    if len(seq) > 0:\n",
    "        sequences_list.append(seq)\n",
    "\n",
    "# Check if sequences_list is empty or has inconsistent shapes\n",
    "if len(sequences_list) > 0:\n",
    "    # Ensure sequences have consistent shapes\n",
    "    sequence_lengths = [seq.shape[1] for seq in sequences_list if len(seq) > 0]\n",
    "\n",
    "    # Stack sequences only if they are consistent\n",
    "    if len(set(sequence_lengths)) == 1:\n",
    "        sequences = np.vstack(sequences_list)\n",
    "    else:\n",
    "        sequences = np.concatenate(sequences_list, axis=0)\n",
    "\n",
    "# Check if sequences are successfully generated\n",
    "if len(sequences) == 0:\n",
    "    raise ValueError(\"No valid sequences generated. Check input data and sequence length.\")\n",
    "\n",
    "print(f\"Generated {len(sequences)} sequences of shape {sequences.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define necessary parameters\n",
    "batch_size = 64\n",
    "seq_len = 24\n",
    "latent_dim = 100\n",
    "epochs = 100\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# Enable eager execution to prevent conversion issues\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Define Generator Model\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(seq_len, latent_dim)),\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='tanh'))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define Discriminator Model\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(seq_len, 1)),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(32),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define Loss Functions\n",
    "def generator_loss(fake_output):\n",
    "    return tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output))\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return tf.reduce_mean(real_loss + fake_loss)\n",
    "\n",
    "# Build and Compile Models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Build models to ensure the optimizers recognize them\n",
    "_ = generator(tf.random.normal([1, seq_len, latent_dim]))\n",
    "_ = discriminator(tf.random.normal([1, seq_len, 1]))\n",
    "\n",
    "# Define Optimizer\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Training Step\n",
    "def train_step(real_data):\n",
    "    # Sample random noise\n",
    "    noise = tf.random.normal([batch_size, seq_len, latent_dim])\n",
    "\n",
    "    # Generate synthetic sequences\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_data = generator(noise, training=True)\n",
    "\n",
    "        # Discriminator predictions\n",
    "        real_output = discriminator(real_data, training=True)\n",
    "        fake_output = discriminator(generated_data, training=True)\n",
    "\n",
    "        # Calculate Losses\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    # Calculate Gradients\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    # Apply Gradients\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# Generate Dummy Sequence Data for Training\n",
    "sequences = np.random.rand(1000, seq_len, 1).astype(np.float32)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    # Sample a batch of real sequences\n",
    "    idx = np.random.randint(0, len(sequences), batch_size)\n",
    "    real_sequences = sequences[idx]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    real_sequences = tf.convert_to_tensor(real_sequences)\n",
    "\n",
    "    # Train the model\n",
    "    gen_loss, disc_loss = train_step(real_sequences)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs} | Generator Loss: {gen_loss.numpy():.4f} | Discriminator Loss: {disc_loss.numpy():.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Generate Sequences for Evaluation after Training\n",
    "# -------------------------------\n",
    "num_samples_to_generate = 1000\n",
    "noise = tf.random.normal([num_samples_to_generate, seq_len, latent_dim])\n",
    "generated_sequences = generator(noise, training=False).numpy()\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Compute statistics for real and generated data\n",
    "# -------------------------------\n",
    "def compute_statistics(sequences):\n",
    "    mean = np.mean(sequences, axis=(0, 1))\n",
    "    std = np.std(sequences, axis=(0, 1))\n",
    "    min_val = np.min(sequences, axis=(0, 1))\n",
    "    max_val = np.max(sequences, axis=(0, 1))\n",
    "    return mean, std, min_val, max_val\n",
    "\n",
    "# Real data statistics\n",
    "real_mean, real_std, real_min, real_max = compute_statistics(sequences)\n",
    "\n",
    "# Generated data statistics\n",
    "generated_mean, generated_std, generated_min, generated_max = compute_statistics(generated_sequences)\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Print Statistics\n",
    "# -------------------------------\n",
    "print(\"Real Data Statistics:\")\n",
    "print(f\"Mean: {float(real_mean):.4f}, Std: {float(real_std):.4f}, Min: {float(real_min):.4f}, Max: {float(real_max):.4f}\")\n",
    "print(\"\\nGenerated Data Statistics:\")\n",
    "print(f\"Mean: {float(generated_mean):.4f}, Std: {float(generated_std):.4f}, Min: {float(generated_min):.4f}, Max: {float(generated_max):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Synthetic Data\n",
    "num_samples = 100\n",
    "noise = tf.random.normal([num_samples, seq_len, latent_dim])\n",
    "synthetic_data = generator(noise)\n",
    "synthetic_data = scaler.inverse_transform(synthetic_data.numpy().reshape(-1, 1)).reshape(num_samples, seq_len)\n",
    "\n",
    "# Visualize Synthetic vs Real Data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(synthetic_data[0], label='Synthetic Data')\n",
    "plt.plot(scaler.inverse_transform(sequences[0].reshape(-1, 1)), label='Real Data')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Meter Reading')\n",
    "plt.title('Comparison of Real vs Synthetic Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Absolute Error between real and synthetic samples\n",
    "real_sample = scaler.inverse_transform(sequences[0].reshape(-1, 1))\n",
    "synthetic_sample = synthetic_data[0]\n",
    "mae = mean_absolute_error(real_sample, synthetic_sample)\n",
    "print(f\"Mean Absolute Error between real and synthetic data: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking using Wasserstein distance and KS test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define necessary parameters\n",
    "batch_size = 64\n",
    "seq_len = 24\n",
    "latent_dim = 100\n",
    "epochs = 100\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# Enable eager execution to prevent conversion issues\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Define Generator Model\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(seq_len, latent_dim)),\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='tanh'))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define Discriminator Model\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(seq_len, 1)),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(32),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define Loss Functions\n",
    "def generator_loss(fake_output):\n",
    "    return tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output))\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return tf.reduce_mean(real_loss + fake_loss)\n",
    "\n",
    "# Build and Compile Models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Build models to ensure the optimizers recognize them\n",
    "_ = generator(tf.random.normal([1, seq_len, latent_dim]))\n",
    "_ = discriminator(tf.random.normal([1, seq_len, 1]))\n",
    "\n",
    "# Define Optimizer\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Training Step\n",
    "def train_step(real_data):\n",
    "    # Sample random noise\n",
    "    noise = tf.random.normal([batch_size, seq_len, latent_dim])\n",
    "\n",
    "    # Generate synthetic sequences\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_data = generator(noise, training=True)\n",
    "\n",
    "        # Discriminator predictions\n",
    "        real_output = discriminator(real_data, training=True)\n",
    "        fake_output = discriminator(generated_data, training=True)\n",
    "\n",
    "        # Calculate Losses\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    # Calculate Gradients\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    # Apply Gradients\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# Generate Dummy Sequence Data for Training\n",
    "sequences = np.random.rand(1000, seq_len, 1).astype(np.float32)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    # Sample a batch of real sequences\n",
    "    idx = np.random.randint(0, len(sequences), batch_size)\n",
    "    real_sequences = sequences[idx]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    real_sequences = tf.convert_to_tensor(real_sequences)\n",
    "\n",
    "    # Train the model\n",
    "    gen_loss, disc_loss = train_step(real_sequences)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs} | Generator Loss: {gen_loss.numpy():.4f} | Discriminator Loss: {disc_loss.numpy():.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Generate Sequences for Evaluation after Training\n",
    "# -------------------------------\n",
    "num_samples_to_generate = 1000\n",
    "noise = tf.random.normal([num_samples_to_generate, seq_len, latent_dim])\n",
    "generated_sequences = generator(noise, training=False).numpy()\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Compute statistics for real and generated data\n",
    "# -------------------------------\n",
    "def compute_statistics(sequences):\n",
    "    mean = np.mean(sequences, axis=(0, 1))\n",
    "    std = np.std(sequences, axis=(0, 1))\n",
    "    min_val = np.min(sequences, axis=(0, 1))\n",
    "    max_val = np.max(sequences, axis=(0, 1))\n",
    "    return mean, std, min_val, max_val\n",
    "\n",
    "# Real data statistics\n",
    "real_mean, real_std, real_min, real_max = compute_statistics(sequences)\n",
    "\n",
    "# Generated data statistics\n",
    "generated_mean, generated_std, generated_min, generated_max = compute_statistics(generated_sequences)\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Print Statistics\n",
    "# -------------------------------\n",
    "print(\"Real Data Statistics:\")\n",
    "print(f\"Mean: {real_mean:.4f}, Std: {real_std:.4f}, Min: {real_min:.4f}, Max: {real_max:.4f}\")\n",
    "print(\"\\nGenerated Data Statistics:\")\n",
    "print(f\"Mean: {generated_mean:.4f}, Std: {generated_std:.4f}, Min: {generated_min:.4f}, Max: {generated_max:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Define Autocorrelation Function\n",
    "# -------------------------------\n",
    "def autocorrelation(sequence):\n",
    "    n = len(sequence)\n",
    "    mean = np.mean(sequence)\n",
    "    var = np.var(sequence)\n",
    "    autocorr = np.correlate(sequence - mean, sequence - mean, mode=\"full\") / (var * n)\n",
    "    return autocorr[n - 1:]\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Sample real and generated sequences for autocorrelation comparison\n",
    "# -------------------------------\n",
    "num_samples_to_plot = 3\n",
    "real_sequences_to_plot = sequences[:num_samples_to_plot]\n",
    "generated_sequences_to_plot = generated_sequences[:num_samples_to_plot]\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Compute and Plot Autocorrelation for Real and Generated Samples\n",
    "# -------------------------------\n",
    "for i in range(num_samples_to_plot):\n",
    "    real_autocorr = autocorrelation(real_sequences_to_plot[i].squeeze())\n",
    "    generated_autocorr = autocorrelation(generated_sequences_to_plot[i].squeeze())\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(real_autocorr[:20], label=\"Real Autocorrelation\")\n",
    "    plt.plot(generated_autocorr[:20], label=\"Generated Autocorrelation\")\n",
    "    plt.title(f\"Autocorrelation - Sequence {i + 1}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iotenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
