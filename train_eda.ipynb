{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_weather_train = pd.read_csv(\"../ashrae-energy-prediction/train.csv\")\n",
    "df_weather_train = pd.read_csv(\"train.csv\")\n",
    "df_weather_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (assuming you've already read it)\n",
    "df_weather_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.histplot(df_weather_train[\"meter_reading\"], bins=50, kde=True)\n",
    "# plt.xlabel(\"Meter Reading\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.title(\"Distribution of Meter Readings\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_weather_train[\"meter\"].value_counts().plot(kind=\"bar\", title=\"Meter Type Distribution\")\n",
    "# plt.xlabel(\"Meter Type\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_train[\"timestamp\"] = pd.to_datetime(df_weather_train[\"timestamp\"])\n",
    "df_weather_train[\"hour\"] = df_weather_train[\"timestamp\"].dt.hour\n",
    "df_weather_train[\"day\"] = df_weather_train[\"timestamp\"].dt.day\n",
    "df_weather_train[\"month\"] = df_weather_train[\"timestamp\"].dt.month\n",
    "df_weather_train[\"year\"] = df_weather_train[\"timestamp\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_weather_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Distribution of meter readings\n",
    "# sns.histplot(df_weather_train['meter_reading'], bins=50, kde=True)\n",
    "# plt.title(\"Distribution of Meter Readings\")\n",
    "# plt.show()\n",
    "\n",
    "# # Trend analysis\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# sns.lineplot(x=df_weather_train['timestamp'], y=df_weather_train['meter_reading'])\n",
    "# plt.title(\"Meter Readings Over Time\")\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Define Generator Network\n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, output_dim)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# # Define Discriminator Network\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# # Training the GAN\n",
    "# def train_gan(real_data, epochs=1000):\n",
    "#     input_dim = real_data.shape[1]\n",
    "#     generator = Generator(input_dim, input_dim)\n",
    "#     discriminator = Discriminator(input_dim)\n",
    "#     optimizer_G = optim.Adam(generator.parameters(), lr=0.001)\n",
    "#     optimizer_D = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "#     loss_function = nn.BCELoss()\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         real_labels = torch.ones(real_data.shape[0], 1)\n",
    "#         fake_labels = torch.zeros(real_data.shape[0], 1)\n",
    "\n",
    "#         # Train Discriminator\n",
    "#         optimizer_D.zero_grad()\n",
    "#         real_predictions = discriminator(real_data)\n",
    "#         real_loss = loss_function(real_predictions, real_labels)\n",
    "\n",
    "#         noise = torch.randn(real_data.shape[0], input_dim)\n",
    "#         fake_data = generator(noise)\n",
    "#         fake_predictions = discriminator(fake_data.detach())\n",
    "#         fake_loss = loss_function(fake_predictions, fake_labels)\n",
    "\n",
    "#         d_loss = real_loss + fake_loss\n",
    "#         d_loss.backward()\n",
    "#         optimizer_D.step()\n",
    "\n",
    "#         # Train Generator\n",
    "#         optimizer_G.zero_grad()\n",
    "#         fake_predictions = discriminator(fake_data)\n",
    "#         g_loss = loss_function(fake_predictions, real_labels)\n",
    "#         g_loss.backward()\n",
    "#         optimizer_G.step()\n",
    "\n",
    "#         if epoch % 100 == 0:\n",
    "#             print(f\"Epoch {epoch}, D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n",
    "\n",
    "#     return generator\n",
    "\n",
    "# # Convert dataset to tensor and train GAN\n",
    "# real_data_tensor = torch.FloatTensor(df_weather_train[['meter_reading']].values)\n",
    "# real_data_tensor = real_data_tensor[:10000]\n",
    "# generator = train_gan(real_data_tensor, epochs=1000)\n",
    "\n",
    "# # Generate synthetic data\n",
    "# noise = torch.randn(10000, 1)\n",
    "# synthetic_meter_readings = generator(noise).detach().numpy()\n",
    "\n",
    "# print(synthetic_meter_readings[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPROVED APPROACH => P-VALUE = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.autograd as autograd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Data Preprocessing\n",
    "# def normalize_data(data):\n",
    "#     scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "#     return scaler, scaler.fit_transform(data)\n",
    "\n",
    "# def denormalize_data(scaler, data):\n",
    "#     return scaler.inverse_transform(data)\n",
    "\n",
    "# # Define Generator Network\n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 128),\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(128, 256),\n",
    "#             nn.BatchNorm1d(256),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(256, 512),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(512, output_dim),\n",
    "#             nn.Tanh()  # Tanh output to match normalized data range\n",
    "#         )\n",
    "#         self.apply(self._weights_init)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "#     def _weights_init(self, m):\n",
    "#         if isinstance(m, nn.Linear):\n",
    "#             nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# # Define Discriminator Network\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 512),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(128, 1)\n",
    "#         )\n",
    "#         self.apply(self._weights_init)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "#     def _weights_init(self, m):\n",
    "#         if isinstance(m, nn.Linear):\n",
    "#             nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# # Compute Gradient Penalty for WGAN-GP\n",
    "# def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "#     alpha = torch.rand(real_samples.size(0), 1).to(real_samples.device)\n",
    "#     interpolated_samples = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "#     interpolated_predictions = discriminator(interpolated_samples)\n",
    "\n",
    "#     gradients = autograd.grad(\n",
    "#         outputs=interpolated_predictions,\n",
    "#         inputs=interpolated_samples,\n",
    "#         grad_outputs=torch.ones_like(interpolated_predictions),\n",
    "#         create_graph=True,\n",
    "#         retain_graph=True,\n",
    "#         only_inputs=True\n",
    "#     )[0]\n",
    "\n",
    "#     gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "#     return gradient_penalty\n",
    "\n",
    "# # Training the WGAN-GP\n",
    "# def train_wgan_gp(real_data, epochs=1000, lambda_gp=10):\n",
    "#     input_dim = real_data.shape[1]\n",
    "#     generator = Generator(input_dim, input_dim)\n",
    "#     discriminator = Discriminator(input_dim)\n",
    "\n",
    "#     optimizer_G = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "#     optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         for _ in range(5):  # Update discriminator more frequently\n",
    "#             optimizer_D.zero_grad()\n",
    "#             real_labels = torch.ones(real_data.shape[0], 1) * 0.9  # Label Smoothing\n",
    "\n",
    "#             noise = torch.randn(real_data.shape[0], input_dim)\n",
    "#             fake_data = generator(noise).detach()\n",
    "#             fake_labels = torch.zeros(real_data.shape[0], 1)\n",
    "\n",
    "#             real_predictions = discriminator(real_data)\n",
    "#             fake_predictions = discriminator(fake_data)\n",
    "\n",
    "#             gradient_penalty = compute_gradient_penalty(discriminator, real_data, fake_data)\n",
    "#             d_loss = -(real_predictions.mean() - fake_predictions.mean()) + lambda_gp * gradient_penalty\n",
    "\n",
    "#             d_loss.backward()\n",
    "#             nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)  # Gradient Clipping\n",
    "#             optimizer_D.step()\n",
    "\n",
    "#         # Train Generator\n",
    "#         optimizer_G.zero_grad()\n",
    "#         noise = torch.randn(real_data.shape[0], input_dim)\n",
    "#         fake_data = generator(noise)\n",
    "#         fake_predictions = discriminator(fake_data)\n",
    "\n",
    "#         g_loss = -fake_predictions.mean()\n",
    "#         g_loss.backward()\n",
    "#         optimizer_G.step()\n",
    "\n",
    "#         if epoch % 100 == 0:\n",
    "#             print(f\"Epoch {epoch}, D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "#     return generator\n",
    "\n",
    "# # Convert dataset to tensor and train WGAN-GP\n",
    "# real_data_np = df_weather_train[['meter_reading']].values\n",
    "# scaler, real_data_normalized = normalize_data(real_data_np)\n",
    "# real_data_tensor = torch.FloatTensor(real_data_normalized[:10000])\n",
    "\n",
    "# generator = train_wgan_gp(real_data_tensor, epochs=1000)\n",
    "\n",
    "# # Generate synthetic data\n",
    "# noise = torch.randn(10000, 1)\n",
    "# synthetic_meter_readings_normalized = generator(noise).detach().numpy()\n",
    "# synthetic_meter_readings = denormalize_data(scaler, synthetic_meter_readings_normalized)\n",
    "\n",
    "# print(synthetic_meter_readings[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Data Preprocessing\n",
    "def normalize_data(data):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    return scaler, scaler.fit_transform(data)\n",
    "\n",
    "def denormalize_data(scaler, data):\n",
    "    return scaler.inverse_transform(data)\n",
    "\n",
    "# Define Generator Network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.apply(self._weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "# Define Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.apply(self._weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "# Compute Gradient Penalty\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "    alpha = torch.rand(real_samples.size(0), 1).to(real_samples.device)\n",
    "    interpolated_samples = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    interpolated_predictions = discriminator(interpolated_samples)\n",
    "    gradients = autograd.grad(\n",
    "        outputs=interpolated_predictions,\n",
    "        inputs=interpolated_samples,\n",
    "        grad_outputs=torch.ones_like(interpolated_predictions),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# Training WGAN-GP\n",
    "def train_wgan_gp(real_data, epochs=3000, lambda_gp=10):\n",
    "    input_dim = real_data.shape[1]\n",
    "    generator = Generator(input_dim, input_dim)\n",
    "    discriminator = Discriminator(input_dim)\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.00003, betas=(0.5, 0.9))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.00003, betas=(0.5, 0.9))\n",
    "\n",
    "    d_losses, g_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(5):  # Train discriminator more frequently\n",
    "            optimizer_D.zero_grad()\n",
    "            noise = torch.randn(real_data.shape[0], input_dim) * 0.7\n",
    "            fake_data = generator(noise).detach()\n",
    "            real_predictions = discriminator(real_data)\n",
    "            fake_predictions = discriminator(fake_data)\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_data, fake_data)\n",
    "            d_loss = -(real_predictions.mean() - fake_predictions.mean()) + lambda_gp * gradient_penalty\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        noise = torch.randn(real_data.shape[0], input_dim) * 0.7\n",
    "        fake_data = generator(noise)\n",
    "        fake_predictions = discriminator(fake_data)\n",
    "        g_loss = -fake_predictions.mean()\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        d_losses.append(d_loss.item())\n",
    "        g_losses.append(g_loss.item())\n",
    "\n",
    "        if epoch % 300 == 0:\n",
    "            print(f\"Epoch {epoch}, D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(d_losses, label='D Loss')\n",
    "    plt.plot(g_losses, label='G Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return generator\n",
    "\n",
    "# Convert dataset to tensor and train WGAN-GP\n",
    "real_data_np = df_weather_train[['meter_reading']].values\n",
    "scaler, real_data_normalized = normalize_data(real_data_np)\n",
    "real_data_tensor = torch.FloatTensor(real_data_normalized[:10000])\n",
    "\n",
    "generator = train_wgan_gp(real_data_tensor, epochs=3000)\n",
    "\n",
    "# Generate synthetic data\n",
    "noise = torch.randn(10000, 1) * 0.7\n",
    "synthetic_meter_readings_normalized = generator(noise).detach().numpy()\n",
    "synthetic_meter_readings = denormalize_data(scaler, synthetic_meter_readings_normalized)\n",
    "\n",
    "print(synthetic_meter_readings[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution based evaluation using histogram and KDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ORIGINAL HISTOGRAM AND KDE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot histogram and KDE for real and synthetic data\n",
    "# plt.figure(figsize=(10,5))\n",
    "# sns.kdeplot(real_data_tensor.numpy().flatten(), label=\"Real Data\", fill=True, alpha=0.5)\n",
    "# sns.kdeplot(synthetic_meter_readings.flatten(), label=\"Synthetic Data\", fill=True, alpha=0.5)\n",
    "# plt.xlabel(\"Meter Reading\")\n",
    "# plt.ylabel(\"Density\")\n",
    "# plt.legend()\n",
    "# plt.title(\"Comparison of Real and Synthetic Meter Readings\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRIAL HISTOGRAM AND KDE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram and KDE for real and synthetic data\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.kdeplot(real_data_tensor.numpy().flatten(), label=\"Real Data\", fill=True, alpha=0.5, warn_singular=False)\n",
    "sns.kdeplot(synthetic_meter_readings.flatten(), label=\"Synthetic Data\", fill=True, alpha=0.5, warn_singular=False)\n",
    "plt.xlabel(\"Meter Reading\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparison of Real and Synthetic Meter Readings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KS Test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# KS Test Evaluation\n",
    "ks_stat, ks_p_value = ks_2samp(real_data_tensor.numpy().flatten(), synthetic_meter_readings.flatten())\n",
    "print(f\"KS Statistic: {ks_stat}\")\n",
    "print(f\"P-Value: {ks_p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "if ks_p_value > 0.05:\n",
    "    print(\"The synthetic data distribution is similar to the real data distribution (Fail to reject H0).\")\n",
    "else:\n",
    "    print(\"The synthetic data distribution differs significantly from the real data distribution (Reject H0).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iotenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
